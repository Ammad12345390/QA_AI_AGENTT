"""
Reporting Agent for AI Test Generation System

Responsibilities:
- Parse unittest execution logs
- Extract test metrics (pass/fail/error)
- Analyze failures using LLM
- Generate Markdown & PDF test reports
"""

import re
from pathlib import Path
from fpdf import FPDF
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

class ReportingAgent:
    def __init__(self, log_path: str):
        self.log_path = Path(log_path)
        if not self.log_path.exists():
            raise FileNotFoundError(f"Log file not found: {log_path}")

        self.client = OpenAI()
        self.results = {}  # stores parsed log summary

    # --------------------------------------------------
    # STEP 1: Parse unittest log
    # --------------------------------------------------
    def parse_unittest_log(self) -> dict:
        content = self.log_path.read_text(encoding="utf-8", errors="ignore")

        # Basic summary dictionary
        self.results = {
            "tests_run": 0,
            "failures": 0,
            "errors": 0,
            "skipped": 0,
            "failed_tests": [],
            "error_tests": [],
            "raw_log": content,
            "status": "UNKNOWN",
            "recommendations": []
        }

        # Total tests
        match = re.search(r"Ran (\d+) tests?", content)
        if match:
            self.results["tests_run"] = int(match.group(1))

        # Failures & Errors
        self.results["failures"] = content.count("FAIL:")
        self.results["errors"] = content.count("ERROR:")

        # Failed test names
        self.results["failed_tests"] = re.findall(r"FAIL: (.+)", content)
        self.results["error_tests"] = re.findall(r"ERROR: (.+)", content)

        # Overall status
        if self.results["failures"] > 0 or self.results["errors"] > 0:
            self.results["status"] = "FAILED"
        else:
            self.results["status"] = "PASSED"

        # Default recommendations
        self.results["recommendations"] = [
            "Remove import-time side effects in main.py.",
            "Parameterize stdout/stderr streams for testability.",
            "Ensure exceptions propagate correctly (do not swallow).",
            "Refactor main.py to separate logic from IO.",
            "Add explicit output verification tests for stdout/stderr behavior.",
            "Add integration/CI checks to avoid import-time side effects."
        ]

        return self.results

    # --------------------------------------------------
    # STEP 2: AI-based Failure Analysis (optional)
    # --------------------------------------------------
    def analyze_with_llm(self, summary: dict) -> str:
        prompt = f"""
You are a Senior QA Automation Engineer.

Below is the result of an automated unittest execution.

Test Summary:
- Total Tests: {summary['tests_run']}
- Failures: {summary['failures']}
- Errors: {summary['errors']}

Failed Tests:
{summary['failed_tests']}

Error Tests:
{summary['error_tests']}

Raw Log:
{summary['raw_log']}

Your Tasks:
1. Identify root causes of failures and errors
2. Categorize issues (logic bug, import error, environment issue, missing mock, syntax issue)
3. Suggest concrete fixes
4. Assess overall code quality (score 1â€“10)
5. Recommend improvements for test coverage

Respond in clear, professional language.
"""

        response = self.client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": "You are an expert QA engineer."},
                {"role": "user", "content": prompt},
            ],
        )

        return response.choices[0].message.content

    # --------------------------------------------------
    # STEP 3: Generate Markdown Report
    # --------------------------------------------------
    def generate_markdown_report(self, summary: dict, ai_analysis: str = "") -> str:
        status = " PASSED" if summary["failures"] == 0 and summary["errors"] == 0 else " FAILED"

        report = f"""
# ðŸ§ª AI Test Execution Report

## ðŸ“Œ Overall Status
*{status}*

---

##  Test Summary
- *Total Tests Run:* {summary['tests_run']}
- *Failures:* {summary['failures']}
- *Errors:* {summary['errors']}

---

##  Failed Tests
{chr(10).join(summary['failed_tests']) if summary['failed_tests'] else "None"}

---

##  Error Tests
{chr(10).join(summary['error_tests']) if summary['error_tests'] else "None"}

---

##  AI Analysis & Recommendations
{ai_analysis or chr(10).join(summary.get('recommendations', []))}

---

## ðŸ›  Generated By
*AI-Powered Test Reporting Agent*
"""
        return report

    # --------------------------------------------------
    # STEP 4: Save Markdown Report
    # --------------------------------------------------
    def save_markdown_report(self, report_content: str, output_path: str = "test_report.md") -> Path:
        output_path = Path(output_path)
        output_path.write_text(report_content, encoding="utf-8")
        return output_path

    # --------------------------------------------------
    # STEP 5: Generate PDF Report
    # --------------------------------------------------
    def generate_pdf_report(self, output_path: str = "test_report.pdf"):
        if not self.results:
            raise ValueError("No parsed results found. Run parse_unittest_log() first.")

        pdf = FPDF()
        pdf.add_page()
        pdf.set_font("Arial", 'B', 16)

        # Title
        pdf.cell(0, 10, "AI Test Execution Report", ln=True, align="C")

        # Overall Status
        pdf.set_font("Arial", '', 12)
        pdf.ln(5)
        pdf.cell(0, 10, f"Overall Status: {self.results['status']}", ln=True)

        # Test Summary
        pdf.ln(5)
        pdf.set_font("Arial", 'B', 14)
        pdf.cell(0, 10, "Test Summary", ln=True)
        pdf.set_font("Arial", '', 12)
        pdf.cell(0, 10, f"Total Tests Run: {self.results['tests_run']}", ln=True)
        pdf.cell(0, 10, f"Failures: {self.results['failures']}", ln=True)
        pdf.cell(0, 10, f"Errors: {self.results['errors']}", ln=True)

        # Error Tests
        pdf.ln(5)
        pdf.set_font("Arial", 'B', 12)
        pdf.cell(0, 10, "Error Tests:", ln=True)
        pdf.set_font("Arial", '', 12)
        if self.results['error_tests']:
            for err in self.results['error_tests']:
                pdf.multi_cell(0, 8, f"- {err}")
        else:
            pdf.cell(0, 10, "None", ln=True)

        # Recommendations
        pdf.ln(5)
        pdf.set_font("Arial", 'B', 12)
        pdf.cell(0, 10, "AI Analysis & Recommendations", ln=True)
        pdf.set_font("Arial", '', 12)
        for rec in self.results.get('recommendations', []):
            pdf.multi_cell(0, 8, f"- {rec}")

        # Save PDF
        pdf.output(output_path)
        print(f"PDF generated successfully at {Path(output_path).resolve()}")

# --------------------------------------------------
# Standalone Execution (Optional)
# --------------------------------------------------
if __name__ == "__main__":
    log_file = input("Enter path to unittest log file: ").strip()

    agent = ReportingAgent(log_file)

    # Parse logs
    agent.parse_unittest_log()

    # Optional AI analysis
    # ai_analysis = agent.analyze_with_llm(agent.results)

    # Generate Markdown
    markdown_report = agent.generate_markdown_report(agent.results)
    md_path = agent.save_markdown_report(markdown_report)
    print(f"Markdown report saved at: {md_path.resolve()}")

    # Generate PDF
    agent.generate_pdf_report("ai_test_report.pdf")
